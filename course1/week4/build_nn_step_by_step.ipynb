{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "build_nn_step_by_step.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO+VPVmTRq5bC0wgQ0XWIjD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pranavkantgaur/Coursera_DL_specialization_from_scratch/blob/master/course1/week4/build_nn_step_by_step.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k89MutsNmCu4",
        "colab_type": "text"
      },
      "source": [
        "## Objectives\n",
        "To be able to build and train NNs for various depth and width. This assignment is not tied to any application, therefore it will be evaluated using test cases. The functions developed in this notebook will be used in the next assignment. Target is to build:\n",
        "\n",
        "\n",
        "*   A 2-layer NN\n",
        "*   A L-layer NN\n",
        "\n",
        "Effectively, I will be able to build and train a L-layer fully connected NN, entirely using Numpy.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhtzkYcq1tTK",
        "colab_type": "text"
      },
      "source": [
        "## Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIaDhxOC2PP1",
        "colab_type": "code",
        "outputId": "283f3e4f-4d99-4186-a9a8-fe5f8570ec2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aySW51Gy2U9c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import matplotlib.pyplot as plt # plotting\n",
        "import h5py # data loading for hdf5 dataset\n",
        "from PIL import Image # for loading your images for processing\n",
        "from scipy import ndimage \n",
        "import os\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDUVDlzM1wx4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# implementing utility function for loading cat vs non-cat datasets\n",
        "def load_dataset():\n",
        "  train_dataset = h5py.File('datasets/train_catvnoncat.h5', \"r\")\n",
        "  test_dataset = h5py.File('datasets/test_catvnoncat.h5', \"r\")\n",
        "  train_set_x = np.array(train_dataset[\"train_set_x\"][:])\n",
        "  train_set_y = np.array(train_dataset[\"train_set_y\"][:])\n",
        "  test_set_x  = np.array(test_dataset[\"test_set_x\"][:])\n",
        "  test_set_y = np.array(test_dataset[\"test_set_y\"][:])\n",
        "  classes = np.array(train_dataset[\"list_classes\"][:])\n",
        "\n",
        "  # lets reshape the arrays\n",
        "  train_set_y = train_set_y.reshape((1, train_set_y.shape[0]))\n",
        "  test_set_y = test_set_y.reshape((1, test_set_y.shape[0]))\n",
        "\n",
        "  return train_set_x, train_set_y, test_set_x, test_set_y, classes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOx2AdCg2Hz9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load training dataset\n",
        "x_train_images, y_train_images, x_test_images, y_test_images, classes = load_dataset()\n",
        "# x_train_images: (m, nx, ny, nc)\n",
        "# y_train_images: (1, m)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDRKs1aj2JOZ",
        "colab_type": "code",
        "outputId": "b64aa483-cd92-4541-92e6-95a5460bbdc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        }
      },
      "source": [
        "# lets inspect the dataset\n",
        "image_id = 25\n",
        "plt.imshow(x_train_images[image_id])\n",
        "print(\"y = \", y_train_images[0][image_id], \"Its a \" + classes[y_train_images[0][image_id]].decode(\"utf-8\") + \" picture!!\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y =  1 Its a cat picture!!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD7CAYAAACscuKmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNE_Gu3_2owh",
        "colab_type": "code",
        "outputId": "fe792ad4-48d8-4522-f8c5-f735e6edd9ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "# lets inspect the shapes of the dataset arrays\n",
        "print(\"Training set shape: \", x_train_images.shape) #  m, nx, ny, nc\n",
        "print(\"Training set labels shape: \", y_train_images.shape) # 1, m\n",
        "print(\"Test set shape: \", x_test_images.shape)\n",
        "print(\"Test set labels shape: \", y_test_images.shape)\n",
        "print(\"Each image is of shape: \", x_train_images[0].shape)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set shape:  (209, 64, 64, 3)\n",
            "Training set labels shape:  (1, 209)\n",
            "Test set shape:  (50, 64, 64, 3)\n",
            "Test set labels shape:  (1, 50)\n",
            "Each image is of shape:  (64, 64, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ttzc-UR52soz",
        "colab_type": "code",
        "outputId": "32c4e504-7477-4a10-fd1f-2add04fe517e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "# Lets flatten the train and test image dataset for training\n",
        "x_train_images_flattened = x_train_images.reshape((x_train_images.shape[0], x_train_images.shape[1] * \n",
        "                                                   x_train_images.shape[2] * \n",
        "                                                   x_train_images.shape[3], 1)) \n",
        "x_train_images_flattened = np.squeeze(x_train_images_flattened)\n",
        "x_train_images_flattened = x_train_images_flattened.T\n",
        "x_test_images_flattened = x_test_images.reshape((x_test_images.shape[0], x_train_images.shape[1] * \n",
        "                                                 x_test_images.shape[2] * \n",
        "                                                 x_test_images.shape[3], 1))\n",
        "\n",
        "x_test_images_flattened = np.squeeze(x_test_images_flattened)\n",
        "x_test_images_flattened = x_test_images_flattened.T\n",
        "\n",
        "# lets print\n",
        "print(\"Flattened version of train images: \", x_train_images_flattened.shape)\n",
        "print(\"Label set for training dataset: \", y_train_images.shape)\n",
        "print(\"Flattened version of test images: \", x_test_images_flattened.shape)\n",
        "print(\"Label set for test dataset: \", y_test_images.shape)\n",
        "\n",
        "\n",
        "print(\"Sanity check after reshaping: \", x_train_images_flattened[0:5, 0])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Flattened version of train images:  (12288, 209)\n",
            "Label set for training dataset:  (1, 209)\n",
            "Flattened version of test images:  (12288, 50)\n",
            "Label set for test dataset:  (1, 50)\n",
            "Sanity check after reshaping:  [17 31 56 22 33]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2d91AXMk2tq8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# normalization\n",
        "x_train_images = x_train_images_flattened / 255.0\n",
        "x_test_images = x_test_images_flattened / 255.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74fPCkiWyCkI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(z):\n",
        "  sig = 1 / (1 + np.exp(-z))\n",
        "  return sig"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOYV-4ivlzfb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TwoLayerNN(object):\n",
        "  def __init__(self, n_hidden_units):\n",
        "    # hyperparameters\n",
        "    self.n_hidden_units = n_hidden_units # len(self.n_hidden_units = number of layers)\n",
        "    self.n_epochs = 0\n",
        "    self.lr = 0\n",
        "    # parameters\n",
        "    self.w1 = 0\n",
        "    self.b1 = 0\n",
        "    self.w2 = 0\n",
        "    self.b2 = 0\n",
        "\n",
        "\n",
        "  def forward(self, X):\n",
        "    # layer 1\n",
        "    z1 = np.dot(self.w1, X) + self.b1\n",
        "    a1 = np.tanh(z1)\n",
        "    \n",
        "    # layer 2\n",
        "    z2 = np.dot(self.w2, a1) + self.b2\n",
        "    a2 = sigmoid(z2)\n",
        "\n",
        "    return a1, a2\n",
        "\n",
        "  def initialize_parameters(self, input_size):\n",
        "    self.w1 = np.random.randn(self.n_hidden_units[0], input_size) * 0.01\n",
        "    self.b1 = np.zeros((self.n_hidden_units[0], 1))\n",
        "    self.w2 = np.random.randn(self.n_hidden_units[1], self.n_hidden_units[0]) * 0.01\n",
        "    self.b2 = np.zeros((self.n_hidden_units[1], 1))\n",
        "    \n",
        "\n",
        "\n",
        "  def backward(self, A1, A2, X, Y):\n",
        "    '''\n",
        "    Notice that the only difference across layers lies in computation of dZ, \n",
        "    rest all can be parameterized on layer ID.\n",
        "    '''\n",
        "    m = X.shape[1]\n",
        "\n",
        "    dZ2 = A2 - Y # output layer\n",
        "    dw2 = np.dot(dZ2, A1.T) / m\n",
        "    db2 = np.mean(dZ2)\n",
        "    \n",
        "    dZ1 = np.dot(self.w2.T, dZ2) * (1 - np.power(A1, 2))\n",
        "    dw1 = np.dot(dZ1, X.T) / m\n",
        "    db1 = np.mean(dZ1)\n",
        "\n",
        "    return dw1, db1, dw2, db2\n",
        "\n",
        "  def update_parameters(self, dw1, db1, dw2, db2):\n",
        "    self.w1 = self.w1 - self.lr * dw1\n",
        "    self.b1 = self.b1 - self.lr * db1\n",
        "    self.w2 = self.w2 - self.lr * dw2\n",
        "    self.b2 = self.b2 - self.lr * db2\n",
        "\n",
        "\n",
        "  def compute_cost(self, A, Y):\n",
        "    cost = -float((np.dot(Y, np.log(A).T) + np.dot(1-Y, np.log(1-A).T))) / Y.shape[1]  \n",
        "    return cost\n",
        "  \n",
        "  def train(self, X, Y, lr = 0.001, n_epochs = 1000):\n",
        "    self.initialize_parameters(X.shape[0]) \n",
        "    self.lr = lr\n",
        "    self.n_epochs = n_epochs\n",
        "\n",
        "    for i in range(n_epochs):\n",
        "      A1, A2  = self.forward(X)\n",
        "      dw1, db1, dw2, db2 = self.backward(A1, A2, X, Y)     \n",
        "      self.update_parameters(dw1, db1, dw2, db2)\n",
        "      cost = self.compute_cost(A2, Y)\n",
        "      if (i % 100 == 0):\n",
        "        print(\"Cost at epoch: \", i, \"is: \", cost)\n",
        "    # save final parameters\n",
        "    try:  \n",
        "      os.mkdir(\"params\")  \n",
        "    except OSError as error:  \n",
        "      print(error)   \n",
        "    np.save(\"params/w1.npy\", self.w1)    \n",
        "    np.save(\"params/b1.npy\", self.b1)\n",
        "    np.save(\"params/w2.npy\", self.w2)\n",
        "    np.save(\"params/b2.npy\", self.b2)\n",
        "\n",
        "  def evaluate(self, X_test, Y_test):\n",
        "    self.w1 = np.load(\"params/w1.npy\")         \n",
        "    self.b1 = np.load(\"params/b1.npy\")\n",
        "    self.w2 = np.load(\"params/w2.npy\")\n",
        "    self.b2 = np.load(\"params/b2.npy\")\n",
        "\n",
        "    _, A_predict = self.forward(X_test)\n",
        "\n",
        "    Y_predict = (A_predict > 0.5)\n",
        "    accuracy = 100 * float(np.dot(Y_test, Y_predict.T) + \n",
        "                           np.dot(1-Y_test, (1-Y_predict).T)) / Y_test.shape[1]\n",
        "    print(\"Accuracy of trained model: \", accuracy)              \n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UsLDWSlcowUr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "twoNN = TwoLayerNN(n_hidden_units = [7, 1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JSuAEWMo04O",
        "colab_type": "code",
        "outputId": "37967d40-0fc1-4c24-b810-50faaa8e6366",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 464
        }
      },
      "source": [
        "twoNN.train(x_train_images, y_train_images, lr = 0.0075, n_epochs = 2500)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cost at epoch:  0 is:  0.6944652964380373\n",
            "Cost at epoch:  100 is:  0.6535740699048482\n",
            "Cost at epoch:  200 is:  0.644585648110276\n",
            "Cost at epoch:  300 is:  0.6369539891521059\n",
            "Cost at epoch:  400 is:  0.6193941733740294\n",
            "Cost at epoch:  500 is:  0.5856055832300676\n",
            "Cost at epoch:  600 is:  0.5416305033255572\n",
            "Cost at epoch:  700 is:  0.4904916360353779\n",
            "Cost at epoch:  800 is:  0.43479541889555573\n",
            "Cost at epoch:  900 is:  0.37815374380125166\n",
            "Cost at epoch:  1000 is:  0.3883964006466481\n",
            "Cost at epoch:  1100 is:  0.33846008312960774\n",
            "Cost at epoch:  1200 is:  0.2960736627136682\n",
            "Cost at epoch:  1300 is:  0.2595824882182009\n",
            "Cost at epoch:  1400 is:  0.22612975182921155\n",
            "Cost at epoch:  1500 is:  0.1933900891321214\n",
            "Cost at epoch:  1600 is:  0.16230566592628967\n",
            "Cost at epoch:  1700 is:  0.1336292055148765\n",
            "Cost at epoch:  1800 is:  0.11300452633516252\n",
            "Cost at epoch:  1900 is:  0.0984541377617943\n",
            "Cost at epoch:  2000 is:  0.08745294356626766\n",
            "Cost at epoch:  2100 is:  0.07773048470495805\n",
            "Cost at epoch:  2200 is:  0.06946321153775063\n",
            "Cost at epoch:  2300 is:  0.062578363099118\n",
            "Cost at epoch:  2400 is:  0.05673108106684982\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKsvpg67o2we",
        "colab_type": "code",
        "outputId": "fa90b6a7-ab11-4846-fe03-4f4ba5a74e50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# train accuracy\n",
        "twoNN.evaluate(x_train_images, y_train_images)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of trained model:  100.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzh7jdQ8_r74",
        "colab_type": "code",
        "outputId": "b4efaca6-6248-42c0-c9d7-3267bd1e80ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# test accuracy\n",
        "twoNN.evaluate(x_test_images, y_test_images)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of trained model:  70.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBGjDbJH0bOe",
        "colab_type": "text"
      },
      "source": [
        "## Generalizing to L-layer NN\n",
        "Now we want to generalize the above 2-layer NN to a L-layer NN, the interface should be such that:\n",
        "\n",
        "* For L=2 and keeping other hyper-parameters same, we should be able to reproduce  the results obtained above. In other words, training a `LLayerNN(n_hidden_units = [7, 1])` network as `lllayerNN.train(x_train_images, y_train_images, lr = 0.0075, n_epochs = 2500)` should result in $72\\%$ test and $100\\%$ train accuracy.\n",
        "*   NN should be implemented by stacking layer along both forward and backward directions:\n",
        "  * forward:  \n",
        "      * Layers:\n",
        "        * hidden layers with tanh/relu activation\n",
        "        * output layer with sigmoid activation\n",
        "      * Input:\n",
        "        * activation from last layer\n",
        "        * weight(and bias) matrix for current layer\n",
        "      * Output:\n",
        "        * activation from current layer        \n",
        "  * backward:\n",
        "      * Layers:\n",
        "        * hidden backward propogation layer\n",
        "        * output bacwkard propogation layer\n",
        "      * Input:\n",
        "        * activation from successive layer\n",
        "        * weight metrix from successive layer\n",
        "        * gradients from successive layer\n",
        "      * Output:\n",
        "        * gradients for current layer, $dw$, $db$ and $dz$.        "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSRgvnZM0hJw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Assumes a binary problem.\n",
        "class LLayerNN(object):\n",
        "  def __init__(self, n_hidden_units):\n",
        "    self.w = [] # (n_hidden_layers + 1)\n",
        "    self.b = [] \n",
        "    self.dw = [] # same as w\n",
        "    self.db = []\n",
        "    self.dz = []\n",
        "    self.a = [] # (n_hidden_layers + 1)\n",
        "    self.lr = 0\n",
        "    self.n_hidden_units = n_hidden_units #total number of layers, n_hidden_units + 1\n",
        "    \n",
        "  def initialize_parameters(self, input_size):\n",
        "    # w[0], b[0]\n",
        "    self.w.append(np.random.rand(self.n_hidden_units[0], input_size) * 0.01)\n",
        "    self.b.append(np.zeros((self.n_hidden_units[0], 1)))\n",
        "    # w[1] to w[n_hidden_layers - 1]\n",
        "    n_hidden_layers = len(self.n_hidden_units)\n",
        "    for i in range(1,  n_hidden_layers): \n",
        "    #(1 for last hidden layer, 1 for output layer)                                         \n",
        "      self.w.append(np.random.randn(self.n_hidden_units[i], \n",
        "                                  self.n_hidden_units[i-1]) * 0.01)\n",
        "      self.b.append(np.zeros((self.n_hidden_units[i], 1)))\n",
        "    # output layer, assuming binary classification\n",
        "    self.w.append(np.random.rand(1, self.n_hidden_units[n_hidden_layers - 1]) * 0.01)\n",
        "    self.b.append(np.zeros((1, 1)))   \n",
        "    \n",
        "    \n",
        "\n",
        "  def forward_propogation(self, layer_id, activation = None):\n",
        "    \n",
        "    z = np.dot(self.w[layer_id], self.a[layer_id - 1]) + self.b[layer_id]      \n",
        "\n",
        "    if (activation == 'tanh'):\n",
        "      self.a.append(np.tanh(z))\n",
        "    if (activation == 'relu'):\n",
        "      self.a.append(np.maximum(0, z))      \n",
        "    if (activation == 'sigmoid'):    \n",
        "      self.a.append(sigmoid(z))\n",
        "\n",
        "  def backward_propogation(self, X, layer_id, activation = None):\n",
        "    \n",
        "    if activation == 'tanh':\n",
        "      da_dz = (1 - np.power(self.a[layer_id], 2))\n",
        "    if activation == 'relu':\n",
        "      da_dz = None #TODO     \n",
        "    # No need of da_dz when activation is sigmoid, dz[0] is actually linear activation from output layer :( \n",
        "    self.dz.append(np.dot(self.w[layer_id+1].T, self.dz[len(self.n_hidden_units) - layer_id - 1]) * da_dz)       \n",
        "    if (layer_id > 0):\n",
        "      self.dw.append(np.dot(self.dz[len(self.n_hidden_units)  - layer_id], self.a[layer_id - 1].T) / X.shape[1])\n",
        "    else:\n",
        "      self.dw.append(np.dot(self.dz[len(self.n_hidden_units)  - layer_id], X.T) / X.shape[1])              \n",
        "    self.db.append(np.mean(self.dz[len(self.n_hidden_units)  - layer_id]))      \n",
        "    \n",
        "\n",
        "  def update_parameters(self, layer_id):\n",
        "    \n",
        "    #t = self.w[layer_id]\n",
        "    self.w[layer_id] = self.w[layer_id] - self.lr * self.dw[len(self.n_hidden_units) - layer_id]\n",
        "    self.b[layer_id] = self.b[layer_id] - self.lr * self.db[len(self.n_hidden_units)  - layer_id]\n",
        "    #print(\"ids: \", layer_id, len(self.n_hidden_units) - layer_id)\n",
        "\n",
        "  def compute_cost(self, Y):\n",
        "    last_layer_id = len(self.n_hidden_units)\n",
        "    cost = -float(np.dot(Y, np.log(self.a[last_layer_id]).T) + np.dot(1 - Y, np.log(1 - self.a[last_layer_id]).T)) / Y.shape[1] #averaged over all samples\n",
        "    return cost  \n",
        "\n",
        "  def train(self, X, Y, learning_rate = 0.001, n_epochs = 1000):\n",
        "      \n",
        "    self.initialize_parameters(X.shape[0]) # initializes parameters for all layers.\n",
        "      \n",
        "    self.lr = learning_rate\n",
        "    n_hidden_layers = len(self.n_hidden_units)\n",
        "      \n",
        "    for i in range(n_epochs):\n",
        "      z = np.dot(self.w[0], X) + self.b[0] # should not be like this :(\n",
        "      self.a.append(np.tanh(z)) \n",
        "      for l in range(1, n_hidden_layers):\n",
        "        self.forward_propogation(l, activation = 'tanh')\n",
        "      # for output layer\n",
        "      self.forward_propogation(n_hidden_layers, activation = 'sigmoid')   \n",
        "      \n",
        "      # backprop for the output layer\n",
        "      self.dz.append(self.a[n_hidden_layers] - Y) # assuming sigmoid activation\n",
        "      self.dw.append(np.dot(self.dz[0], self.a[n_hidden_layers - 1].T) / X.shape[1])\n",
        "      self.db.append(np.mean(self.dz[0]))\n",
        "\n",
        "      for l in range(n_hidden_layers  - 1 , -1, -1):\n",
        "        self.backward_propogation(X, l, activation = 'tanh') \n",
        "      \n",
        "      for l in range(n_hidden_layers + 1):\n",
        "        self.update_parameters(l)  \n",
        "      cost = self.compute_cost(Y) # ends the epoch\n",
        "      if (i % 1000 == 0):\n",
        "        print(\"Cost at epoch: \", i, \"is: \", cost)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQ3hBkSFEoS-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "llayernn = LLayerNN(n_hidden_units = [20, 7, 5]) # 3 hidden layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuNqITSnErnG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "llayernn.train(x_train_images, y_train_images, learning_rate = 0.0075, n_epochs = 3000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hIl4vX91YvM",
        "colab_type": "text"
      },
      "source": [
        "## Implementation based on dictionary\n",
        "In the above implmentation, parameters are shared as global attributes of `LLayerNN` class. The implementation gets messy because `dz, dw` gets calculated in reverse order with respect to `a`, hence maintaining them in lists results in indexing chaos.  \n",
        "In the following implementation of`LLayerNN` class we will share the parameters of the network using `dicts`, or the string-key based dictionary of parameters, similar to Coursera code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tirR0G_J2Rad",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LLayerNN(object):\n",
        "  def __init__(self, n_hidden_units):\n",
        "    self.parameters = {}\n",
        "    self.n_hidden_units = n_hidden_units\n",
        "    self.n_layers = len(self.n_hidden_units) + 1\n",
        "    self.lr = -1\n",
        "\n",
        "\n",
        "  def initialize_parameters(self, input_size):\n",
        "    self.parameters[\"w0\"] = np.random.randn(self.n_hidden_units[0],\n",
        "                                       input_size) * 0.01\n",
        "    self.parameters[\"b0\"] = np.zeros((self.n_hidden_units[0], 1))                                       \n",
        "    for i in range(1, self.n_layers - 1):\n",
        "      self.parameters[\"w\" + str(i)] = np.random.randn(self.n_hidden_units[i],\n",
        "                                            self.n_hidden_units[i - 1]) * 0.01\n",
        "      self.parameters[\"b\" + str(i)] = np.zeros((self.n_hidden_units[i], 1))\n",
        "    self.parameters[\"w\" + str(self.n_layers - 1)] = np.random.randn(1,\n",
        "                                 self.n_hidden_units[self.n_layers - 2]) * 0.01\n",
        "    self.parameters[\"b\" + str(self.n_layers - 1)] = np.zeros((1, 1))                                                                                         \n",
        "      \n",
        "\n",
        "  def forward(self, layer_id, X, activation = None):\n",
        "    # linear block\n",
        "    if (layer_id > 0):\n",
        "      z = np.dot(self.parameters[\"w\" + str(layer_id)], \n",
        "      self.parameters[\"a\" + str(layer - 1)]) + self.parameters[\"b\" + str(layer_id)]\n",
        "    else:\n",
        "      z = np.dot(self.parameters[\"w\" + str(layer_id)], \n",
        "                 X) + self.parameters[\"b\" + str(layer_id)]\n",
        "\n",
        "    # activation block \n",
        "    if (activation == 'tanh'):\n",
        "      self.parameters[\"a\" + str(layer_id)] = np.tanh(z)\n",
        "    if (activation == 'relu'):\n",
        "      self.parameters[\"a\" + str(layer_id)] = np.maximum(0, z)      \n",
        "    if (activation == 'sigmoid'):\n",
        "      self.parameters['a' + str(layer_id)] = sigmoid(z)  \n",
        "    \n",
        "\n",
        "  def backward(self, layer_id, X, Y, activation = None):\n",
        "    # activation block\n",
        "    if (activation == 'sigmoid'):\n",
        "      self.parameters[\"dz\" + str(layer_id)] = self.parameters[\"a\" + str(layer_id)] - Y\n",
        "    if (activation == 'relu'):\n",
        "      da_dz = [1 if (self.parameters[\"a\" + str(layer_id)] > 0) else 0] \n",
        "      assert(da_dz.shape == self.parameters[\"a\" + str(layer_id)].shape)\n",
        "      self.parameters[\"dz\" + str(layer_id)] = np.dot(self.parameters[\"w\" + str(layer_id + 1)].T, self.parameters[\"dz\" + str(layer_id + 1)]) * da_dz\n",
        "    if (activation == 'tanh'):\n",
        "      da_dz = 1 - np.power(self.parameters[\"a\" + str(layer_id)], 2)\n",
        "      self.parameters[\"dz\" + str(layer_id)] = np.dot(self.parameters[\"w\" + str(layer_id + 1)].T, self.parameters[\"dz\" + str(layer_id + 1)]) * da_dz\n",
        "      \n",
        "    #linear block\n",
        "    if(layer_id > 0):\n",
        "      self.parameters[\"dw\" + str(layer_id)] = np.dot(self.parameters[\"dz\" + str(layer_id)], \n",
        "                                                   self.parameters[\"a\" + str(layer_id - 1)])\n",
        "    else:\n",
        "      self.parameters[\"dw0\"] = np.dot(self.parameters[\"dz\" + str(layer_id)], \n",
        "                                                   X)\n",
        "    self.parameters[\"db\" + str(layer_id)] = np.mean(self.parameters[\"dz\" + str(layer_id)])\n",
        "  \n",
        "\n",
        "  def update_parameters(self, layer_id):\n",
        "    self.parameters[\"w\" + str(layer_id)] = self.parameters[\"w\" + str(layer_id)] + self.lr * self.parameters[\"dw\" + str(layer_id)]\n",
        "\n",
        "    self.parameters[\"b\" + str(layer_id)] = self.parameters[\"b\" + str(layer_id)] + self.lr * self.parameters[\"db\" + str(layer_id)]\n",
        "\n",
        "\n",
        "  def compute_cost(self, Y):\n",
        "    cost = -float((np.dot(Y, np.log(self.parameters[\"a\" + str(self.n_layers)]).T) + np.dot(1 - Y, np.log(1 - self.parameters[\"a\" + str(self.n_layers)]).T))) / Y.shape[1]                \n",
        "\n",
        "\n",
        "  def train(self, X, Y, learning_rate = 0.001, n_epochs = 3000):\n",
        "    self.initialize_parameters(X.shape[0])\n",
        "    self.lr = learning_rate\n",
        "    for i in range(n_epochs):\n",
        "      # forward\n",
        "      for l in range(self.n_layers):\n",
        "        if (l == self.n_layers - 1):\n",
        "          self.forward(l,X, activation='sigmoid')\n",
        "        else:  \n",
        "          self.forward(l,X, activation='relu')\n",
        "      # backward\n",
        "      for l in range(self.n_layers - 1, -1, -1):\n",
        "        self.backward(l, X, Y)\n",
        "        self.update_parameters(l)\n",
        "      cost = self.compute_cost(Y)\n",
        "      if (i % 1000 == 0):\n",
        "        print(\"Cost at epoch: \", i, \"is: \", cost)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkAVJgR22rzn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "llayernn = LLayerNN(n_hidden_units = [20, 7, 5]) # 3 hidden layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2u5L19ii2wGb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "outputId": "8a966acf-982f-45e6-8569-b5451784678d"
      },
      "source": [
        "llayernn.train(x_train_images, y_train_images, learning_rate = 0.0075, \n",
        "               n_epochs = 3000)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-70d4cae70f76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m llayernn.train(x_train_images, y_train_images, learning_rate = 0.0075, \n\u001b[0;32m----> 2\u001b[0;31m                n_epochs = 3000)\n\u001b[0m",
            "\u001b[0;32m<ipython-input-39-c7f545b44a17>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, Y, learning_rate, n_epochs)\u001b[0m\n\u001b[1;32m     79\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m       \u001b[0;31m# backward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_layers\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-c7f545b44a17>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, layer_id, X, activation)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m       z = np.dot(self.parameters[\"w\" + str(layer_id)], \n\u001b[0;32m---> 29\u001b[0;31m                  X) + self.parameters[\"b\" + str(layer_id)]\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m# activation block\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: shapes (20,209) and (12288,209) not aligned: 209 (dim 1) != 12288 (dim 0)"
          ]
        }
      ]
    }
  ]
}