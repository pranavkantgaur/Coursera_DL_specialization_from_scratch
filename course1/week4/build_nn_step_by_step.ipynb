{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "build_nn_step_by_step.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOyuTyXwlDyKyz2bIKwZdWt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pranavkantgaur/Coursera_DL_specialization_from_scratch/blob/master/course1/week4/build_nn_step_by_step.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k89MutsNmCu4",
        "colab_type": "text"
      },
      "source": [
        "## Objectives\n",
        "To be able to build and train NNs for various depth and width. This assignment is not tied to any application, therefore it will be evaluated using test cases. The functions developed in this notebook will be used in the next assignment. Target is to build:\n",
        "\n",
        "\n",
        "*   A 2-layer NN\n",
        "*   A L-layer NN\n",
        "\n",
        "Effectively, I will be able to build and train a L-layer fully connected NN, entirely using Numpy.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhtzkYcq1tTK",
        "colab_type": "text"
      },
      "source": [
        "## Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIaDhxOC2PP1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aySW51Gy2U9c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import matplotlib.pyplot as plt # plotting\n",
        "import h5py # data loading for hdf5 dataset\n",
        "from PIL import Image # for loading your images for processing\n",
        "from scipy import ndimage \n",
        "import os\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDUVDlzM1wx4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# implementing utility function for loading cat vs non-cat datasets\n",
        "def load_dataset():\n",
        "  train_dataset = h5py.File('datasets/train_catvnoncat.h5', \"r\")\n",
        "  test_dataset = h5py.File('datasets/test_catvnoncat.h5', \"r\")\n",
        "  train_set_x = np.array(train_dataset[\"train_set_x\"][:])\n",
        "  train_set_y = np.array(train_dataset[\"train_set_y\"][:])\n",
        "  test_set_x  = np.array(test_dataset[\"test_set_x\"][:])\n",
        "  test_set_y = np.array(test_dataset[\"test_set_y\"][:])\n",
        "  classes = np.array(train_dataset[\"list_classes\"][:])\n",
        "\n",
        "  # lets reshape the arrays\n",
        "  train_set_y = train_set_y.reshape((1, train_set_y.shape[0]))\n",
        "  test_set_y = test_set_y.reshape((1, test_set_y.shape[0]))\n",
        "\n",
        "  return train_set_x, train_set_y, test_set_x, test_set_y, classes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOx2AdCg2Hz9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load training dataset\n",
        "x_train_images, y_train_images, x_test_images, y_test_images, classes = load_dataset()\n",
        "# x_train_images: (m, nx, ny, nc)\n",
        "# y_train_images: (1, m)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDRKs1aj2JOZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# lets inspect the dataset\n",
        "image_id = 25\n",
        "plt.imshow(x_train_images[image_id])\n",
        "print(\"y = \", y_train_images[0][image_id], \"Its a \" + classes[y_train_images[0][image_id]].decode(\"utf-8\") + \" picture!!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNE_Gu3_2owh",
        "colab_type": "code",
        "outputId": "a17b0913-e066-4c97-80c5-01b887bef8aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "# lets inspect the shapes of the dataset arrays\n",
        "print(\"Training set shape: \", x_train_images.shape) #  m, nx, ny, nc\n",
        "print(\"Training set labels shape: \", y_train_images.shape) # 1, m\n",
        "print(\"Test set shape: \", x_test_images.shape)\n",
        "print(\"Test set labels shape: \", y_test_images.shape)\n",
        "print(\"Each image is of shape: \", x_train_images[0].shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set shape:  (209, 64, 64, 3)\n",
            "Training set labels shape:  (1, 209)\n",
            "Test set shape:  (50, 64, 64, 3)\n",
            "Test set labels shape:  (1, 50)\n",
            "Each image is of shape:  (64, 64, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ttzc-UR52soz",
        "colab_type": "code",
        "outputId": "9b5f62c2-d338-4612-fe3a-81d3e46fa714",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "# Lets flatten the train and test image dataset for training\n",
        "x_train_images_flattened = x_train_images.reshape((x_train_images.shape[0], x_train_images.shape[1] * \n",
        "                                                   x_train_images.shape[2] * \n",
        "                                                   x_train_images.shape[3], 1)) \n",
        "x_train_images_flattened = np.squeeze(x_train_images_flattened)\n",
        "x_train_images_flattened = x_train_images_flattened.T\n",
        "x_test_images_flattened = x_test_images.reshape((x_test_images.shape[0], x_train_images.shape[1] * \n",
        "                                                 x_test_images.shape[2] * \n",
        "                                                 x_test_images.shape[3], 1))\n",
        "\n",
        "x_test_images_flattened = np.squeeze(x_test_images_flattened)\n",
        "x_test_images_flattened = x_test_images_flattened.T\n",
        "\n",
        "# lets print\n",
        "print(\"Flattened version of train images: \", x_train_images_flattened.shape)\n",
        "print(\"Label set for training dataset: \", y_train_images.shape)\n",
        "print(\"Flattened version of test images: \", x_test_images_flattened.shape)\n",
        "print(\"Label set for test dataset: \", y_test_images.shape)\n",
        "\n",
        "\n",
        "print(\"Sanity check after reshaping: \", x_train_images_flattened[0:5, 0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Flattened version of train images:  (12288, 209)\n",
            "Label set for training dataset:  (1, 209)\n",
            "Flattened version of test images:  (12288, 50)\n",
            "Label set for test dataset:  (1, 50)\n",
            "Sanity check after reshaping:  [17 31 56 22 33]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2d91AXMk2tq8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# normalization\n",
        "x_train_images = x_train_images_flattened / 255.0\n",
        "x_test_images = x_test_images_flattened / 255.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74fPCkiWyCkI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(z):\n",
        "  sig = 1 / (1 + np.exp(-z))\n",
        "  return sig"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOYV-4ivlzfb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TwoLayerNN(object):\n",
        "  def __init__(self, n_hidden_units):\n",
        "    # hyperparameters\n",
        "    self.n_hidden_units = n_hidden_units # len(self.n_hidden_units = number of layers)\n",
        "    self.n_epochs = 0\n",
        "    self.lr = 0\n",
        "    # parameters\n",
        "    self.w1 = 0\n",
        "    self.b1 = 0\n",
        "    self.w2 = 0\n",
        "    self.b2 = 0\n",
        "\n",
        "\n",
        "  def forward(self, X):\n",
        "    # layer 1\n",
        "    z1 = np.dot(self.w1, X) + self.b1\n",
        "    a1 = np.tanh(z1)\n",
        "    \n",
        "    # layer 2\n",
        "    z2 = np.dot(self.w2, a1) + self.b2\n",
        "    a2 = sigmoid(z2)\n",
        "\n",
        "    return a1, a2\n",
        "\n",
        "  def initialize_parameters(self, input_size):\n",
        "    self.w1 = np.random.randn(self.n_hidden_units[0], input_size) * 0.01\n",
        "    self.b1 = np.zeros((self.n_hidden_units[0], 1))\n",
        "    self.w2 = np.random.randn(self.n_hidden_units[1], self.n_hidden_units[0]) * 0.01\n",
        "    self.b2 = np.zeros((self.n_hidden_units[1], 1))\n",
        "    \n",
        "\n",
        "\n",
        "  def backward(self, A1, A2, X, Y):\n",
        "    '''\n",
        "    Notice that the only difference across layers lies in computation of dZ, \n",
        "    rest all can be parameterized on layer ID.\n",
        "    '''\n",
        "    m = X.shape[1]\n",
        "\n",
        "    dZ2 = A2 - Y # output layer\n",
        "    dw2 = np.dot(dZ2, A1.T) / m\n",
        "    db2 = np.mean(dZ2)\n",
        "    \n",
        "    dZ1 = np.dot(self.w2.T, dZ2) * (1 - np.power(A1, 2))\n",
        "    dw1 = np.dot(dZ1, X.T) / m\n",
        "    db1 = np.mean(dZ1)\n",
        "\n",
        "    return dw1, db1, dw2, db2\n",
        "\n",
        "  def update_parameters(self, dw1, db1, dw2, db2):\n",
        "    self.w1 = self.w1 - self.lr * dw1\n",
        "    self.b1 = self.b1 - self.lr * db1\n",
        "    self.w2 = self.w2 - self.lr * dw2\n",
        "    self.b2 = self.b2 - self.lr * db2\n",
        "\n",
        "\n",
        "  def compute_cost(self, A, Y):\n",
        "    cost = -float((np.dot(Y, np.log(A).T) + np.dot(1-Y, np.log(1-A).T))) / Y.shape[1]  \n",
        "    return cost\n",
        "  \n",
        "  def train(self, X, Y, lr = 0.001, n_epochs = 1000):\n",
        "    self.initialize_parameters(X.shape[0]) \n",
        "    self.lr = lr\n",
        "    self.n_epochs = n_epochs\n",
        "\n",
        "    for i in range(n_epochs):\n",
        "      A1, A2  = self.forward(X)\n",
        "      dw1, db1, dw2, db2 = self.backward(A1, A2, X, Y)     \n",
        "      self.update_parameters(dw1, db1, dw2, db2)\n",
        "      cost = self.compute_cost(A2, Y)\n",
        "      if (i % 100 == 0):\n",
        "        print(\"Cost at epoch: \", i, \"is: \", cost)\n",
        "    # save final parameters\n",
        "    try:  \n",
        "      os.mkdir(\"params\")  \n",
        "    except OSError as error:  \n",
        "      print(error)   \n",
        "    np.save(\"params/w1.npy\", self.w1)    \n",
        "    np.save(\"params/b1.npy\", self.b1)\n",
        "    np.save(\"params/w2.npy\", self.w2)\n",
        "    np.save(\"params/b2.npy\", self.b2)\n",
        "\n",
        "  def evaluate(self, X_test, Y_test):\n",
        "    self.w1 = np.load(\"params/w1.npy\")         \n",
        "    self.b1 = np.load(\"params/b1.npy\")\n",
        "    self.w2 = np.load(\"params/w2.npy\")\n",
        "    self.b2 = np.load(\"params/b2.npy\")\n",
        "\n",
        "    _, A_predict = self.forward(X_test)\n",
        "\n",
        "    Y_predict = (A_predict > 0.5)\n",
        "    accuracy = 100 * float(np.dot(Y_test, Y_predict.T) + \n",
        "                           np.dot(1-Y_test, (1-Y_predict).T)) / Y_test.shape[1]\n",
        "    print(\"Accuracy of trained model: \", accuracy)              \n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UsLDWSlcowUr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "twoNN = TwoLayerNN(n_hidden_units = [7, 1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JSuAEWMo04O",
        "colab_type": "code",
        "outputId": "0f86146c-78f2-40c6-c706-76765eb09d39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        }
      },
      "source": [
        "twoNN.train(x_train_images, y_train_images, lr = 0.0075, n_epochs = 2500)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cost at epoch:  0 is:  0.6960598883051189\n",
            "Cost at epoch:  100 is:  0.6536072145590778\n",
            "Cost at epoch:  200 is:  0.6433395957351822\n",
            "Cost at epoch:  300 is:  0.6322147317400523\n",
            "Cost at epoch:  400 is:  0.6090849610699225\n",
            "Cost at epoch:  500 is:  0.5693893707912853\n",
            "Cost at epoch:  600 is:  0.5192816899228848\n",
            "Cost at epoch:  700 is:  0.46402643499119944\n",
            "Cost at epoch:  800 is:  0.4054218086958313\n",
            "Cost at epoch:  900 is:  0.3483431158020084\n",
            "Cost at epoch:  1000 is:  0.3661978135585861\n",
            "Cost at epoch:  1100 is:  0.31715161465176755\n",
            "Cost at epoch:  1200 is:  0.27375951665337434\n",
            "Cost at epoch:  1300 is:  0.23799486072934836\n",
            "Cost at epoch:  1400 is:  0.20560113547168046\n",
            "Cost at epoch:  1500 is:  0.17546035130293572\n",
            "Cost at epoch:  1600 is:  0.14661803198282103\n",
            "Cost at epoch:  1700 is:  0.12003370522266767\n",
            "Cost at epoch:  1800 is:  0.10260553857061602\n",
            "Cost at epoch:  1900 is:  0.09075074425096824\n",
            "Cost at epoch:  2000 is:  0.08105347562643966\n",
            "Cost at epoch:  2100 is:  0.07293505112329696\n",
            "Cost at epoch:  2200 is:  0.06605656311623632\n",
            "Cost at epoch:  2300 is:  0.06017227422551496\n",
            "Cost at epoch:  2400 is:  0.055096349768998874\n",
            "[Errno 17] File exists: 'params'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKsvpg67o2we",
        "colab_type": "code",
        "outputId": "92d5e422-a2cc-4bb6-e47a-f1715ff0d457",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# train accuracy\n",
        "twoNN.evaluate(x_train_images, y_train_images)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of trained model:  100.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzh7jdQ8_r74",
        "colab_type": "code",
        "outputId": "cf42a16f-16e1-4747-de7d-4b3fd3edda5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# test accuracy\n",
        "twoNN.evaluate(x_test_images, y_test_images)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of trained model:  72.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBGjDbJH0bOe",
        "colab_type": "text"
      },
      "source": [
        "## Generalizing to L-layer NN\n",
        "Now we want to generalize the above 2-layer NN to a L-layer NN, the interface should be such that:\n",
        "\n",
        "* For L=2 and keeping other hyper-parameters same, we should be able to reproduce  the results obtained above. In other words, training a `LLayerNN(n_hidden_units = [7, 1])` network as `lllayerNN.train(x_train_images, y_train_images, lr = 0.0075, n_epochs = 2500)` should result in $72\\%$ test and $100\\%$ train accuracy.\n",
        "*   NN should be implemented by stacking layer along both forward and backward directions:\n",
        "  * forward:  \n",
        "      * Layers:\n",
        "        * hidden layers with tanh/relu activation\n",
        "        * output layer with sigmoid activation\n",
        "      * Input:\n",
        "        * activation from last layer\n",
        "        * weight(and bias) matrix for current layer\n",
        "      * Output:\n",
        "        * activation from current layer        \n",
        "  * backward:\n",
        "      * Layers:\n",
        "        * hidden backward propogation layer\n",
        "        * output bacwkard propogation layer\n",
        "      * Input:\n",
        "        * activation from successive layer\n",
        "        * weight metrix from successive layer\n",
        "        * gradients from successive layer\n",
        "      * Output:\n",
        "        * gradients for current layer, $dw$, $db$ and $dz$.        "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSRgvnZM0hJw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Assumes a binary problem.\n",
        "class LLayerNN(object):\n",
        "  def __init__(self, n_hidden_units):\n",
        "    self.w = []\n",
        "    self.b = []\n",
        "    self.dw = []\n",
        "    self.db = []\n",
        "    self.a = [] # activations across layers\n",
        "    self.lr = None\n",
        "    self.n_hidden_units = n_hidden_units '''total number of layers, \n",
        "                                          n_hidden_units + 1'''\n",
        "    \n",
        "  def initialize_parameters(self, input_size):\n",
        "    # layer 0\n",
        "    self.w.append(np.random.rand(self.n_hidden_units[0], input_size)) * 0.01\n",
        "    self.b.append(np.zeros((self.n_hidden_units[0], 1)))\n",
        "    # layer 1 to L \n",
        "    n_hidden_layers = len(self.n_hidden_units)\n",
        "    for i in range(1,  n_hidden_layers): ''' (1 for last hidden layer, \n",
        "                                                    1 for output layer)'''\n",
        "      self.w.append(np.random.randn(self.n_hidden_units[i], \n",
        "                                  self.n_hidden_units[i-1]) * 0.01)\n",
        "      self.b.append(np.zeros(self.n_hidden_units[i], 1))\n",
        "    # output layer, assuming binary classification\n",
        "    self.w.append(np.random.rand(1, self.n_hidden_units[n_hidden_layers - 1])) * 0.01\n",
        "    self.b.append(np.zeros((1, 1)))   \n",
        "    # w: n_hidden_layers + 1       \n",
        "\n",
        "  def forward_propogation(self, layer_id, activation = None):\n",
        "    z = np.dot(self.w[layer_id - 1], self.a[layer_id - 1]) + self.b[layer_id  -1]  # NOT READABLE!!\n",
        "    if (activation == 'tanh'):\n",
        "      self.a[layer_id] = np.tanh(z)\n",
        "    if (activation == 'relu'):\n",
        "      self.a[layer_id] = np.max(0, z)      \n",
        "    if (activation == 'sigmoid'):    \n",
        "      self.a[layer_id] = sigmoid(z)\n",
        "\n",
        "   def backward_propogation(self, Y, layer_id, activation = None):\n",
        "     if activation == 'tanh':\n",
        "       da_dz = (1 - np.power(self.a[layer_id], 2))\n",
        "     if activation == 'relu':\n",
        "       da_dz = None      \n",
        "     if activation == 'sigmoid':\n",
        "       da_dz = np.dot(self.a[layer_id], 1 - self.a[layer_id]) \n",
        "     if (layer_id == L): # output layer\n",
        "       self.dz[layer_id] = self.a[layer_id] - Y\n",
        "     else:\n",
        "       self.dz[layer_id] = np.dot(self.w[l+1].T, self.dz[l+1]) * da_dz       \n",
        "     self.dw[layer_id] = np.dot(self.dz[l], self.a[l - 1]) / m\n",
        "     self.db[layer_id] = np.mean(self.dz[l])      \n",
        "    \n",
        "\n",
        "    def update_parameters(self, layer_id):\n",
        "      self.w[layer_id] = self.w[layer_id] - self.lr * self.dw[layer_id]\n",
        "      self.b[layer_id] = self.b[layer_id] - self.lr * self.db[layer_id]\n",
        "\n",
        "    def compute_cost(self, Y):\n",
        "      last_layer_id = len(num_hidden_units + 1)\n",
        "      cost = -(np.dot(Y, np.log(self.a[last_layer_id])).T) + np.dot(1 - Y, \n",
        "             np.log(1 - self.a[last_layer_id]))) / Y.shape[1] '''averaged over \n",
        "                                                              all samples'''\n",
        "      return cost  \n",
        "\n",
        "    def train(self, X, Y, learning_rate = 0.001, n_epochs = 1000):\n",
        "      \n",
        "      self.initialize_parameters() # initializes parameters for all layers.\n",
        "      \n",
        "      self.lr = learning_rate\n",
        "      n_hidden_layers = len(self.n_hidden_units)\n",
        "      \n",
        "      self.a.append(X) # activation from input layer, a[0]\n",
        "      for i in range(n_epochs):\n",
        "        for l in range(1, n_hidden_layers + 1):\n",
        "          self.forward_propogation(l, activation = 'relu')\n",
        "        # for output layer\n",
        "        self.forward_propogation(n_hidden_layers, activation = 'sigmoid')   \n",
        "        \n",
        "        # backprop for the output layer\n",
        "        self.backward_propogation(n_hidden_layers, activation = 'sigmoid') \n",
        "        \n",
        "        for l in range(n_hidden_layers  - 1 , 0, -1):\n",
        "          self.backward_propogation(l, activation = 'tanh') \n",
        "          self.update_parameters(l) # updates weights for layer 'l'\n",
        "        cost = self.compute_cost() # ends the epoch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQ3hBkSFEoS-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "llayernn = LLayerNN()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuNqITSnErnG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "llayernn.train(x_train_images, y_train_images)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GErxkzqnEwMq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "llayernn.evaluate(x_test_images, y_test_images)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}