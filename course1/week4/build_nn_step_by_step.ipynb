{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "build_nn_step_by_step.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pranavkantgaur/Coursera_DL_specialization_from_scratch/blob/master/course1/week4/build_nn_step_by_step.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "k89MutsNmCu4"
      },
      "source": [
        "## Objectives\n",
        "To be able to build and train NNs for various depth and width. This assignment is not tied to any application, therefore it will be evaluated using test cases. The functions developed in this notebook will be used in the next assignment. Target is to build:\n",
        "\n",
        "\n",
        "*   A 2-layer NN\n",
        "*   A L-layer NN\n",
        "\n",
        "Effectively, I will be able to build and train a L-layer fully connected NN, entirely using Numpy.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LhtzkYcq1tTK"
      },
      "source": [
        "## Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fIaDhxOC2PP1",
        "outputId": "5d67ec57-3a3a-41ff-9cab-c72a365f7edb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aySW51Gy2U9c",
        "colab": {}
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import matplotlib.pyplot as plt # plotting\n",
        "import h5py # data loading for hdf5 dataset\n",
        "from PIL import Image # for loading your images for processing\n",
        "from scipy import ndimage \n",
        "import os\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cDUVDlzM1wx4",
        "colab": {}
      },
      "source": [
        "# implementing utility function for loading cat vs non-cat datasets\n",
        "def load_dataset():\n",
        "  train_dataset = h5py.File('datasets/train_catvnoncat.h5', \"r\")\n",
        "  test_dataset = h5py.File('datasets/test_catvnoncat.h5', \"r\")\n",
        "  train_set_x = np.array(train_dataset[\"train_set_x\"][:])\n",
        "  train_set_y = np.array(train_dataset[\"train_set_y\"][:])\n",
        "  test_set_x  = np.array(test_dataset[\"test_set_x\"][:])\n",
        "  test_set_y = np.array(test_dataset[\"test_set_y\"][:])\n",
        "  classes = np.array(train_dataset[\"list_classes\"][:])\n",
        "\n",
        "  # lets reshape the arrays\n",
        "  train_set_y = train_set_y.reshape((1, train_set_y.shape[0]))\n",
        "  test_set_y = test_set_y.reshape((1, test_set_y.shape[0]))\n",
        "\n",
        "  return train_set_x, train_set_y, test_set_x, test_set_y, classes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bOx2AdCg2Hz9",
        "colab": {}
      },
      "source": [
        "# load training dataset\n",
        "x_train_images, y_train_images, x_test_images, y_test_images, classes = load_dataset()\n",
        "# x_train_images: (m, nx, ny, nc)\n",
        "# y_train_images: (1, m)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cDRKs1aj2JOZ",
        "outputId": "38f13ba3-d9af-4581-9ca0-5bc9be7a5bd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "# lets inspect the dataset\n",
        "image_id = 25\n",
        "plt.imshow(x_train_images[image_id])\n",
        "print(\"y = \", y_train_images[0][image_id], \"Its a \" + classes[y_train_images[0][image_id]].decode(\"utf-8\") + \" picture!!\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y =  1 Its a cat picture!!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vNE_Gu3_2owh",
        "outputId": "5e900223-9b7b-43ea-833c-2c737706cfa1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "source": [
        "# lets inspect the shapes of the dataset arrays\n",
        "print(\"Training set shape: \", x_train_images.shape) #  m, nx, ny, nc\n",
        "print(\"Training set labels shape: \", y_train_images.shape) # 1, m\n",
        "print(\"Test set shape: \", x_test_images.shape)\n",
        "print(\"Test set labels shape: \", y_test_images.shape)\n",
        "print(\"Each image is of shape: \", x_train_images[0].shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set shape:  (209, 64, 64, 3)\n",
            "Training set labels shape:  (1, 209)\n",
            "Test set shape:  (50, 64, 64, 3)\n",
            "Test set labels shape:  (1, 50)\n",
            "Each image is of shape:  (64, 64, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ttzc-UR52soz",
        "outputId": "3c2896df-3c93-4106-8b4f-e9a0431cb752",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "source": [
        "# Lets flatten the train and test image dataset for training\n",
        "x_train_images_flattened = x_train_images.reshape((x_train_images.shape[0], x_train_images.shape[1] * \n",
        "                                                   x_train_images.shape[2] * \n",
        "                                                   x_train_images.shape[3], 1)) \n",
        "x_train_images_flattened = np.squeeze(x_train_images_flattened)\n",
        "x_train_images_flattened = x_train_images_flattened.T\n",
        "x_test_images_flattened = x_test_images.reshape((x_test_images.shape[0], x_train_images.shape[1] * \n",
        "                                                 x_test_images.shape[2] * \n",
        "                                                 x_test_images.shape[3], 1))\n",
        "\n",
        "x_test_images_flattened = np.squeeze(x_test_images_flattened)\n",
        "x_test_images_flattened = x_test_images_flattened.T\n",
        "\n",
        "# lets print\n",
        "print(\"Flattened version of train images: \", x_train_images_flattened.shape)\n",
        "print(\"Label set for training dataset: \", y_train_images.shape)\n",
        "print(\"Flattened version of test images: \", x_test_images_flattened.shape)\n",
        "print(\"Label set for test dataset: \", y_test_images.shape)\n",
        "\n",
        "\n",
        "print(\"Sanity check after reshaping: \", x_train_images_flattened[0:5, 0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Flattened version of train images:  (12288, 209)\n",
            "Label set for training dataset:  (1, 209)\n",
            "Flattened version of test images:  (12288, 50)\n",
            "Label set for test dataset:  (1, 50)\n",
            "Sanity check after reshaping:  [17 31 56 22 33]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2d91AXMk2tq8",
        "colab": {}
      },
      "source": [
        "# normalization\n",
        "x_train_images = x_train_images_flattened / 255.0\n",
        "x_test_images = x_test_images_flattened / 255.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "74fPCkiWyCkI",
        "colab": {}
      },
      "source": [
        "def sigmoid(z):\n",
        "  sig = 1 / (1 + np.exp(-z))\n",
        "  return sig"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QOYV-4ivlzfb",
        "colab": {}
      },
      "source": [
        "class TwoLayerNN(object):\n",
        "  def __init__(self, n_hidden_units):\n",
        "    # hyperparameters\n",
        "    self.n_hidden_units = n_hidden_units # len(self.n_hidden_units = number of layers)\n",
        "    self.n_epochs = 0\n",
        "    self.lr = 0\n",
        "    # parameters\n",
        "    self.w1 = 0\n",
        "    self.b1 = 0\n",
        "    self.w2 = 0\n",
        "    self.b2 = 0\n",
        "\n",
        "\n",
        "  def forward(self, X):\n",
        "    # layer 1\n",
        "    z1 = np.dot(self.w1, X) + self.b1\n",
        "    a1 = np.tanh(z1)\n",
        "    \n",
        "    # layer 2\n",
        "    z2 = np.dot(self.w2, a1) + self.b2\n",
        "    a2 = sigmoid(z2)\n",
        "\n",
        "    return a1, a2\n",
        "\n",
        "  def initialize_parameters(self, input_size):\n",
        "    self.w1 = np.random.randn(self.n_hidden_units[0], input_size) * 0.01\n",
        "    self.b1 = np.zeros((self.n_hidden_units[0], 1))\n",
        "    self.w2 = np.random.randn(self.n_hidden_units[1], self.n_hidden_units[0]) * 0.01\n",
        "    self.b2 = np.zeros((self.n_hidden_units[1], 1))\n",
        "    \n",
        "\n",
        "\n",
        "  def backward(self, A1, A2, X, Y):\n",
        "    '''\n",
        "    Notice that the only difference across layers lies in computation of dZ, \n",
        "    rest all can be parameterized on layer ID.\n",
        "    '''\n",
        "    m = X.shape[1]\n",
        "\n",
        "    dZ2 = A2 - Y # output layer\n",
        "    dw2 = np.dot(dZ2, A1.T) / m\n",
        "    db2 = np.mean(dZ2)\n",
        "    \n",
        "    dZ1 = np.dot(self.w2.T, dZ2) * (1 - np.power(A1, 2))\n",
        "    dw1 = np.dot(dZ1, X.T) / m\n",
        "    db1 = np.mean(dZ1)\n",
        "\n",
        "    return dw1, db1, dw2, db2\n",
        "\n",
        "  def update_parameters(self, dw1, db1, dw2, db2):\n",
        "    self.w1 = self.w1 - self.lr * dw1\n",
        "    self.b1 = self.b1 - self.lr * db1\n",
        "    self.w2 = self.w2 - self.lr * dw2\n",
        "    self.b2 = self.b2 - self.lr * db2\n",
        "\n",
        "\n",
        "  def compute_cost(self, A, Y):\n",
        "    cost = -float((np.dot(Y, np.log(A).T) + np.dot(1-Y, np.log(1-A).T))) / Y.shape[1]  \n",
        "    return cost\n",
        "  \n",
        "  def train(self, X, Y, lr = 0.001, n_epochs = 1000):\n",
        "    self.initialize_parameters(X.shape[0]) \n",
        "    self.lr = lr\n",
        "    self.n_epochs = n_epochs\n",
        "\n",
        "    for i in range(n_epochs):\n",
        "      A1, A2  = self.forward(X)\n",
        "      dw1, db1, dw2, db2 = self.backward(A1, A2, X, Y)     \n",
        "      self.update_parameters(dw1, db1, dw2, db2)\n",
        "      cost = self.compute_cost(A2, Y)\n",
        "      if (i % 100 == 0):\n",
        "        print(\"Cost at epoch: \", i, \"is: \", cost)\n",
        "    # save final parameters\n",
        "    try:  \n",
        "      os.mkdir(\"params\")  \n",
        "    except OSError as error:  \n",
        "      print(error)   \n",
        "    np.save(\"params/w1.npy\", self.w1)    \n",
        "    np.save(\"params/b1.npy\", self.b1)\n",
        "    np.save(\"params/w2.npy\", self.w2)\n",
        "    np.save(\"params/b2.npy\", self.b2)\n",
        "\n",
        "  def evaluate(self, X_test, Y_test):\n",
        "    self.w1 = np.load(\"params/w1.npy\")         \n",
        "    self.b1 = np.load(\"params/b1.npy\")\n",
        "    self.w2 = np.load(\"params/w2.npy\")\n",
        "    self.b2 = np.load(\"params/b2.npy\")\n",
        "\n",
        "    _, A_predict = self.forward(X_test)\n",
        "\n",
        "    Y_predict = (A_predict > 0.5)\n",
        "    accuracy = 100 * float(np.dot(Y_test, Y_predict.T) + \n",
        "                           np.dot(1-Y_test, (1-Y_predict).T)) / Y_test.shape[1]\n",
        "    print(\"Accuracy of trained model: \", accuracy)              \n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UsLDWSlcowUr",
        "colab": {}
      },
      "source": [
        "twoNN = TwoLayerNN(n_hidden_units = [7, 1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-JSuAEWMo04O",
        "outputId": "ce8d3d6f-42e9-43c3-d113-88f13bc70783",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        }
      },
      "source": [
        "twoNN.train(x_train_images, y_train_images, lr = 0.0075, n_epochs = 2500)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cost at epoch:  0 is:  0.6925732572537425\n",
            "Cost at epoch:  100 is:  0.6496883030701376\n",
            "Cost at epoch:  200 is:  0.6412165650881596\n",
            "Cost at epoch:  300 is:  0.6278579495819233\n",
            "Cost at epoch:  400 is:  0.6015550519072044\n",
            "Cost at epoch:  500 is:  0.5621130154731159\n",
            "Cost at epoch:  600 is:  0.5134103983299865\n",
            "Cost at epoch:  700 is:  0.45991319207777753\n",
            "Cost at epoch:  800 is:  0.4042203172119329\n",
            "Cost at epoch:  900 is:  0.3511061137237675\n",
            "Cost at epoch:  1000 is:  0.3623072589718516\n",
            "Cost at epoch:  1100 is:  0.31654212145440463\n",
            "Cost at epoch:  1200 is:  0.27903809146365904\n",
            "Cost at epoch:  1300 is:  0.2443853473870132\n",
            "Cost at epoch:  1400 is:  0.21082335948320144\n",
            "Cost at epoch:  1500 is:  0.17870469456304985\n",
            "Cost at epoch:  1600 is:  0.14233455415208388\n",
            "Cost at epoch:  1700 is:  0.12362876465814868\n",
            "Cost at epoch:  1800 is:  0.10985738636964573\n",
            "Cost at epoch:  1900 is:  0.09829515114787374\n",
            "Cost at epoch:  2000 is:  0.08825006109665892\n",
            "Cost at epoch:  2100 is:  0.0794286273937023\n",
            "Cost at epoch:  2200 is:  0.07174949057488234\n",
            "Cost at epoch:  2300 is:  0.06517910832643618\n",
            "Cost at epoch:  2400 is:  0.059555757114188944\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hKsvpg67o2we",
        "outputId": "18a5f538-f28b-4a7d-b7e2-980f02ea81e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# train accuracy\n",
        "twoNN.evaluate(x_train_images, y_train_images)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of trained model:  100.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hzh7jdQ8_r74",
        "outputId": "98a2f406-478a-41d2-bcda-fd201e1edbc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# test accuracy\n",
        "twoNN.evaluate(x_test_images, y_test_images)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of trained model:  72.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gBGjDbJH0bOe"
      },
      "source": [
        "## Generalizing to L-layer NN\n",
        "Now we want to generalize the above 2-layer NN to a L-layer NN, the interface should be such that:\n",
        "\n",
        "* For L=2 and keeping other hyper-parameters same, we should be able to reproduce  the results obtained above. In other words, training a `LLayerNN(n_hidden_units = [7, 1])` network as `lllayerNN.train(x_train_images, y_train_images, lr = 0.0075, n_epochs = 2500)` should result in $72\\%$ test and $100\\%$ train accuracy.\n",
        "*   NN should be implemented by stacking layer along both forward and backward directions:\n",
        "  * forward:  \n",
        "      * Layers:\n",
        "        * hidden layers with tanh/relu activation\n",
        "        * output layer with sigmoid activation\n",
        "      * Input:\n",
        "        * activation from last layer\n",
        "        * weight(and bias) matrix for current layer\n",
        "      * Output:\n",
        "        * activation from current layer        \n",
        "  * backward:\n",
        "      * Layers:\n",
        "        * hidden backward propogation layer\n",
        "        * output bacwkard propogation layer\n",
        "      * Input:\n",
        "        * activation from successive layer\n",
        "        * weight metrix from successive layer\n",
        "        * gradients from successive layer\n",
        "      * Output:\n",
        "        * gradients for current layer, $dw$, $db$ and $dz$.        "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RSRgvnZM0hJw",
        "colab": {}
      },
      "source": [
        "# Assumes a binary problem.\n",
        "class LLayerNN(object):\n",
        "  def __init__(self, n_hidden_units):\n",
        "    self.w = [] # (n_hidden_layers + 1)\n",
        "    self.b = [] \n",
        "    self.dw = [] # same as w\n",
        "    self.db = []\n",
        "    self.dz = []\n",
        "    self.a = [] # (n_hidden_layers + 1)\n",
        "    self.lr = 0\n",
        "    self.n_hidden_units = n_hidden_units #total number of layers, n_hidden_units + 1\n",
        "    \n",
        "  def initialize_parameters(self, input_size):\n",
        "    # w[0], b[0]\n",
        "    self.w.append(np.random.rand(self.n_hidden_units[0], input_size) * 0.01)\n",
        "    self.b.append(np.zeros((self.n_hidden_units[0], 1)))\n",
        "    # w[1] to w[n_hidden_layers - 1]\n",
        "    n_hidden_layers = len(self.n_hidden_units)\n",
        "    for i in range(1,  n_hidden_layers): \n",
        "    #(1 for last hidden layer, 1 for output layer)                                         \n",
        "      self.w.append(np.random.randn(self.n_hidden_units[i], \n",
        "                                  self.n_hidden_units[i-1]) * 0.01)\n",
        "      self.b.append(np.zeros((self.n_hidden_units[i], 1)))\n",
        "    # output layer, assuming binary classification\n",
        "    self.w.append(np.random.rand(1, self.n_hidden_units[n_hidden_layers - 1]) * 0.01)\n",
        "    self.b.append(np.zeros((1, 1)))   \n",
        "    \n",
        "    \n",
        "\n",
        "  def forward_propogation(self, layer_id, activation = None):\n",
        "    \n",
        "    z = np.dot(self.w[layer_id], self.a[layer_id - 1]) + self.b[layer_id]      \n",
        "\n",
        "    if (activation == 'tanh'):\n",
        "      self.a.append(np.tanh(z))\n",
        "    if (activation == 'relu'):\n",
        "      self.a.append(np.maximum(0, z))      \n",
        "    if (activation == 'sigmoid'):    \n",
        "      self.a.append(sigmoid(z))\n",
        "\n",
        "  def backward_propogation(self, X, layer_id, activation = None):\n",
        "    \n",
        "    if activation == 'tanh':\n",
        "      da_dz = (1 - np.power(self.a[layer_id], 2))\n",
        "    if activation == 'relu':\n",
        "      da_dz = None #TODO     \n",
        "    # No need of da_dz when activation is sigmoid, dz[0] is actually linear activation from output layer :( \n",
        "    self.dz.append(np.dot(self.w[layer_id+1].T, self.dz[len(self.n_hidden_units) - layer_id - 1]) * da_dz)       \n",
        "    if (layer_id > 0):\n",
        "      self.dw.append(np.dot(self.dz[len(self.n_hidden_units)  - layer_id], self.a[layer_id - 1].T) / X.shape[1])\n",
        "    else:\n",
        "      self.dw.append(np.dot(self.dz[len(self.n_hidden_units)  - layer_id], X.T) / X.shape[1])              \n",
        "    self.db.append(np.mean(self.dz[len(self.n_hidden_units)  - layer_id]))      \n",
        "    \n",
        "\n",
        "  def update_parameters(self, layer_id):\n",
        "    \n",
        "    #t = self.w[layer_id]\n",
        "    self.w[layer_id] = self.w[layer_id] - self.lr * self.dw[len(self.n_hidden_units) - layer_id]\n",
        "    self.b[layer_id] = self.b[layer_id] - self.lr * self.db[len(self.n_hidden_units)  - layer_id]\n",
        "    #print(\"ids: \", layer_id, len(self.n_hidden_units) - layer_id)\n",
        "\n",
        "  def compute_cost(self, Y):\n",
        "    last_layer_id = len(self.n_hidden_units)\n",
        "    cost = -float(np.dot(Y, np.log(self.a[last_layer_id]).T) + np.dot(1 - Y, np.log(1 - self.a[last_layer_id]).T)) / Y.shape[1] #averaged over all samples\n",
        "    return cost  \n",
        "\n",
        "  def train(self, X, Y, learning_rate = 0.001, n_epochs = 1000):\n",
        "      \n",
        "    self.initialize_parameters(X.shape[0]) # initializes parameters for all layers.\n",
        "      \n",
        "    self.lr = learning_rate\n",
        "    n_hidden_layers = len(self.n_hidden_units)\n",
        "      \n",
        "    for i in range(n_epochs):\n",
        "      z = np.dot(self.w[0], X) + self.b[0] # should not be like this :(\n",
        "      self.a.append(np.tanh(z)) \n",
        "      for l in range(1, n_hidden_layers):\n",
        "        self.forward_propogation(l, activation = 'tanh')\n",
        "      # for output layer\n",
        "      self.forward_propogation(n_hidden_layers, activation = 'sigmoid')   \n",
        "      \n",
        "      # backprop for the output layer\n",
        "      self.dz.append(self.a[n_hidden_layers] - Y) # assuming sigmoid activation\n",
        "      self.dw.append(np.dot(self.dz[0], self.a[n_hidden_layers - 1].T) / X.shape[1])\n",
        "      self.db.append(np.mean(self.dz[0]))\n",
        "\n",
        "      for l in range(n_hidden_layers  - 1 , -1, -1):\n",
        "        self.backward_propogation(X, l, activation = 'tanh') \n",
        "      \n",
        "      for l in range(n_hidden_layers + 1):\n",
        "        self.update_parameters(l)  \n",
        "      cost = self.compute_cost(Y) # ends the epoch\n",
        "      if (i % 1000 == 0):\n",
        "        print(\"Cost at epoch: \", i, \"is: \", cost)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DQ3hBkSFEoS-",
        "colab": {}
      },
      "source": [
        "llayernn = LLayerNN(n_hidden_units = [20, 7, 5]) # 3 hidden layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xuNqITSnErnG",
        "outputId": "34a564ae-b85e-4bee-a95c-c2a7eace00e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "llayernn.train(x_train_images, y_train_images, learning_rate = 0.0075, n_epochs = 3000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cost at epoch:  0 is:  0.6931491328891869\n",
            "Cost at epoch:  1000 is:  0.6931491328891869\n",
            "Cost at epoch:  2000 is:  0.6931491328891869\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1hIl4vX91YvM"
      },
      "source": [
        "## Implementation based on dictionary\n",
        "In the above implmentation, parameters are shared as global attributes of `LLayerNN` class. The implementation gets messy because `dz, dw` gets calculated in reverse order with respect to `a`, hence maintaining them in lists results in indexing chaos.  \n",
        "In the following implementation of`LLayerNN` class we will share the parameters of the network using `dicts`, or the string-key based dictionary of parameters, similar to the Coursera assignments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tirR0G_J2Rad",
        "outputId": "ee28b234-5b10-41c4-baa5-4e07b69a8856",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "'''\n",
        "Assumes one-node output layer.\n",
        "'''\n",
        "class LLayerNN(object):\n",
        "  def __init__(self, n_hidden_units):\n",
        "    self.parameters = {}\n",
        "    self.n_hidden_units = n_hidden_units\n",
        "    self.n_layers = len(self.n_hidden_units) + 1\n",
        "    self.lr = -1\n",
        "\n",
        "\n",
        "  def initialize_parameters(self, input_size):\n",
        "    scale_factor = 0.01\n",
        "    self.parameters[\"w0\"] = np.random.randn(self.n_hidden_units[0], \\\n",
        "                                       input_size) * scale_factor\n",
        "    self.parameters[\"b0\"] = np.zeros((self.n_hidden_units[0], 1))                                       \n",
        "    for i in range(1, self.n_layers - 1):\n",
        "      self.parameters[\"w\" + str(i)] = np.random.randn(self.n_hidden_units[i],\n",
        "                                            self.n_hidden_units[i - 1]) * \\\n",
        "                                            scale_factor\n",
        "      self.parameters[\"b\" + str(i)] = np.zeros((self.n_hidden_units[i], 1))\n",
        "      #print(\"W\", i, \" is: \", self.parameters[\"w\" + str(i)])\n",
        "      \n",
        "    self.parameters[\"w\" + str(self.n_layers - 1)] = np.random.randn(1,\n",
        "                                 self.n_hidden_units[self.n_layers - 2]) \\\n",
        "                                 * scale_factor\n",
        "    self.parameters[\"b\" + str(self.n_layers - 1)] = np.zeros((1, 1))                                                                                         \n",
        "      \n",
        "\n",
        "  def forward(self, layer_id, X, activation = None):\n",
        "    # linear block\n",
        "    if (layer_id > 0):\n",
        "      z = np.dot(self.parameters[\"w\" + str(layer_id)], \\\n",
        "                 self.parameters[\"a\" + str(layer_id - 1)]) + \\\n",
        "                 self.parameters[\"b\" + str(layer_id)] # broadcasting!!\n",
        "    else:\n",
        "      z = np.dot(self.parameters[\"w\" + str(layer_id)], \n",
        "                 X) + self.parameters[\"b\" + str(layer_id)]\n",
        "\n",
        "    # activation block \n",
        "    if (activation == 'tanh'):\n",
        "      self.parameters[\"a\" + str(layer_id)] = np.tanh(z)\n",
        "      #print(\"tanh\")\n",
        "    if (activation == 'relu'):\n",
        "      self.parameters[\"a\" + str(layer_id)] = np.maximum(0, z)   \n",
        "      #print(\"layer: \", layer_id, \" relu forward\")   \n",
        "    if (activation == 'sigmoid'):\n",
        "      self.parameters['a' + str(layer_id)] = sigmoid(z)  \n",
        "      #print(\"layer: \", layer_id,\" sigmoid forward\") \n",
        "    \n",
        "    #print(\"b[\" + str(layer_id) + \"] = \", self.parameters[\"b\" + str(layer_id)].shape)\n",
        "    #print(\"z[\" + str(layer_id) + \"] = \", z.shape)\n",
        "    #print(\"a[\" + str(layer_id) + \"] = \", self.parameters[\"a\" + str(layer_id)].shape)\n",
        "\n",
        "  def backward(self, layer_id, X, Y, activation = None):\n",
        "    # activation block\n",
        "    if (layer_id == self.n_layers - 1 and activation == 'sigmoid'):#output layer\n",
        "      self.parameters[\"dz\" + str(layer_id)] = \\\n",
        "      self.parameters[\"a\" + str(layer_id)] - Y # (1, M) - (1, M) = (1,M)\n",
        "      #print(\"layer: \", layer_id,\" sigmoid backwards\")\n",
        "    elif (layer_id < self.n_layers - 1 ):\n",
        "      if (activation == 'relu'):\n",
        "        da_dz = np.squeeze(np.asarray([self.parameters[\"a\" + str(layer_id)] > 0])) # (n_units, M)\n",
        "        #print(\"da_dz: \", da_dz.shape)\n",
        "        #print(\"layer: \", layer_id, \" relu backwards\") \n",
        "      elif (activation == 'tanh'): # untestedt TODO\n",
        "        da_dz = 1 - np.power(self.parameters[\"a\" + str(layer_id)], 2)\n",
        "      elif (activation == 'sigmoid'): # untested, TODO\n",
        "        da_dz = self.parameters[\"a\" + str(layer_id)] * \\\n",
        "      (1 - self.parameters[\"a\" + str(layer_id)])\n",
        "      else:\n",
        "        raise ValueError(\"Invalid choice of arguments\") # untested, TODO\n",
        "\n",
        "      dl_da = np.dot(self.parameters[\"w\" + str(layer_id + 1)].T, \\\n",
        "                   self.parameters[\"dz\" + str(layer_id + 1)])\n",
        "      #print(\"dl_da: \", dl_da.shape) # (n_units[l] , n_units[l+1]) X (n_units[l+1], M)\n",
        "      self.parameters[\"dz\" + str(layer_id)] =  dl_da * da_dz # (n_units[l] , M) X (n_units[l], M)\n",
        "      #print(\"dz[\" + str(layer_id) + \"]\", self.parameters[\"dz\" + str(layer_id)].shape)\n",
        "\n",
        "    assert(self.parameters[\"dz\" + str(layer_id)].shape == \\\n",
        "           self.parameters[\"a\" + str(layer_id)].shape)   \n",
        "    #linear block\n",
        "    if(layer_id > 0):\n",
        "      self.parameters[\"dw\" + str(layer_id)] = \\\n",
        "      np.dot(self.parameters[\"dz\" + str(layer_id)], \\\n",
        "             self.parameters[\"a\" + str(layer_id - 1)].T) / Y.shape[1] # across all examples\n",
        "    else: # input layer\n",
        "      self.parameters[\"dw0\"] = \\\n",
        "      np.dot(self.parameters[\"dz\" + str(layer_id)], X.T)  / Y.shape[1] # across all examples\n",
        "  \n",
        "    #print(\"dw: \", self.parameters[\"dw\" + str(layer_id)].shape)\n",
        "    #print(\"w: \", self.parameters[\"w\" + str(layer_id)].shape)\n",
        "    assert(self.parameters[\"dw\" + str(layer_id)].shape == \\\n",
        "           self.parameters[\"w\" + str(layer_id)].shape)\n",
        "    \n",
        "    self.parameters[\"db\" + str(layer_id)] = \\\n",
        "    np.mean(self.parameters[\"dz\" + str(layer_id)], axis = -1).T.\\\n",
        "    reshape(self.parameters[\"b\" + str(layer_id)].shape)  # across all examples\n",
        "    \n",
        "    #print(\"db: \", self.parameters[\"db\" + str(layer_id)].shape)\n",
        "    #print(\"b: \", self.parameters[\"b\" + str(layer_id)].shape)\n",
        "    #assert(self.parameters[\"db\" + str(layer_id)].shape == \\\n",
        "    #       self.parameters[\"b\" + str(layer_id)].shape)\n",
        "\n",
        "    \n",
        "  def update_parameters(self, layer_id):\n",
        "    self.parameters[\"w\" + str(layer_id)] = self.parameters[\"w\" + str(layer_id)] \\\n",
        "    - self.lr * self.parameters[\"dw\" + str(layer_id)]\n",
        "\n",
        "    self.parameters[\"b\" + str(layer_id)] = self.parameters[\"b\" + str(layer_id)] \\\n",
        "    - self.lr * self.parameters[\"db\" + str(layer_id)]\n",
        "\n",
        "\n",
        "  def compute_cost(self, Y):\n",
        "    cost = \\\n",
        "    -float(np.dot(Y, np.log(self.parameters[\"a\" + str(self.n_layers-1)]).T) \\\n",
        "           + np.dot(1 - Y, np.log(1 - self.parameters[\"a\" + \\\n",
        "                                                      str(self.n_layers-1)]).T)) \\\n",
        "                   / Y.shape[1]                \n",
        "    return cost\n",
        "\n",
        "  def train(self, X, Y, learning_rate = 0.001, n_epochs = 3000):\n",
        "    np.random.seed(1)\n",
        "    self.initialize_parameters(X.shape[0])\n",
        "    self.lr = learning_rate\n",
        "    \n",
        "    for i in range(n_epochs):\n",
        "    #for i in range(2):\n",
        "      # forward\n",
        "      for l in range(self.n_layers):\n",
        "        if (l == self.n_layers - 1):\n",
        "          self.forward(l,X, activation='sigmoid')\n",
        "        else:  \n",
        "          self.forward(l,X, activation='relu')\n",
        "      # backward\n",
        "      for l in range(self.n_layers - 1, -1, -1):\n",
        "        if(l == self.n_layers -1):\n",
        "          self.backward(l, X, Y,activation='sigmoid')\n",
        "        else:  \n",
        "          self.backward(l,X,Y,activation='relu')\n",
        "      for l in range(self.n_layers):\n",
        "        self.update_parameters(l)\n",
        "      cost = self.compute_cost(Y)\n",
        "      if (i % 100 == 0):\n",
        "        print(\"Cost at epoch: \", i, \"is: \", cost)  \n",
        "'''\n",
        "  def grad_check(X, Y, ephsilon, threshold):\n",
        "    # set random param values for testing\n",
        "    initialize_parameters(X.shape[0])\n",
        "    \n",
        "    # compute number of params\n",
        "    n_params = 0\n",
        "    for l in range(self.n_layers):\n",
        "      for i in range(self.parameters[\"w\" + str(l)].shape[0]):\n",
        "        for j in range(self.parameters[\"w\" + str(l).shape[1]]):\n",
        "          n_params += 1 \n",
        "    \n",
        "    \n",
        "    J_plus = np.array((n_params, 1)) \n",
        "    J_minus = np.array((n_params, 1))\n",
        "    j_plus_index = 0 # counters\n",
        "    j_minus_index = 0\n",
        "\n",
        "    # for each parameter of the network\n",
        "    for l in range(self.n_layers):\n",
        "      # compute J plus\n",
        "      # \"w\" parameter\n",
        "      for i in range(self.parameters[\"w\" + str(l)].shape[0]):\n",
        "        for j in range(self.parameters[\"w\" + str(l).shape[1]]):\n",
        "          self.parameters[\"w\" + str(l)][i][j] += ephsilon  \n",
        "          forward(l, X, Y)\n",
        "          J_plus[j_index++] = compute_cost(Y)\n",
        "          self.parameters[\"w\" + str(l)][i][j] -= ephsilon\n",
        "      # \"b\" parameter\n",
        "      for i in range(self.parameters[\"b\" + str(l)].shape[0]):\n",
        "        for j in range(self.parameters[\"b\" + str(l).shape[1]]):\n",
        "          self.parameters[\"b\" + str(l)][i][j] += ephsilon  \n",
        "          forward(l, X, Y)\n",
        "          J_plus[j_index++] = compute_cost(Y)\n",
        "          self.parameters[\"b\" + str(l)][i][j] -= ephsilon\n",
        "      # compute J minus\n",
        "      # \"w\" parameter\n",
        "      for i in range(self.parameters[\"w\" + str(l)].shape[0]):\n",
        "        for j in range(self.parameters[\"w\" + str(l).shape[1]]):\n",
        "          self.parameters[\"w\" + str(l)][i][j] -= ephsilon  \n",
        "          forward(l, X, Y)\n",
        "          J_minus[j_minus_index++] = compute_cost(Y)\n",
        "          self.parameters[\"w\" + str(l)][i][j] += ephsilon\n",
        "      # \"b\" parameter\n",
        "      for i in range(self.parameters[\"b\" + str(l)].shape[0]):\n",
        "        for j in range(self.parameters[\"b\" + str(l).shape[1]]):\n",
        "          self.parameters[\"b\" + str(l)][i][j] -= ephsilon  \n",
        "          forward(l, X, Y)\n",
        "          J_minus[j_minus_index++] = compute_cost(Y)\n",
        "          self.parameters[\"b\" + str(l)][i][j] += ephsilon\n",
        "    # compute numerical gradient      \n",
        "    grad_numerical = J_plus - J_minus / 2 * ephsilon\n",
        "\n",
        "    # compute backprop gradient\n",
        "    grad_index = 0\n",
        "    for l in range(self.n_layers):\n",
        "      forward(l, X, Y)\n",
        "      backward(l, X, Y)\n",
        "      # collect gradients in an array\n",
        "      # for 'w'\n",
        "      for i in range(self.parameters[\"dw\" + str(l)].shape[0]):\n",
        "        for j in range(self.parameters[\"dw\" + str(l)].shape[1]):\n",
        "          grad_brackprop[grad_index++] = self.parameters[\"dw\" + str(l)][i][j]   \n",
        "      # similarly for 'b'\n",
        "      for i in range(self.parameters[\"db\" + str(l)].shape[0]):\n",
        "        for j in range(self.parameters[\"db\" + str(l)].shape[1]):\n",
        "          grad_brackprop[grad_index] = self.parameters[\"db\" + str(l)][i][j]  \n",
        "    \n",
        "    # compute difference\n",
        "    diff_num = # TODO\n",
        "    diff_den = # TODO\n",
        "    diff = diff_num / diff_den\n",
        "\n",
        "    if diff < threshold:\n",
        "      print(\"Gradients are fine!!\")\n",
        "    else:\n",
        "      print(\"Please check gradients!!\") \n",
        " '''      "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n  def grad_check(X, Y, ephsilon, threshold):\\n    # set random param values for testing\\n    initialize_parameters(X.shape[0])\\n    \\n    # compute number of params\\n    n_params = 0\\n    for l in range(self.n_layers):\\n      for i in range(self.parameters[\"w\" + str(l)].shape[0]):\\n        for j in range(self.parameters[\"w\" + str(l).shape[1]]):\\n          n_params += 1 \\n    \\n    \\n    J_plus = np.array((n_params, 1)) \\n    J_minus = np.array((n_params, 1))\\n    j_plus_index = 0 # counters\\n    j_minus_index = 0\\n\\n    # for each parameter of the network\\n    for l in range(self.n_layers):\\n      # compute J plus\\n      # \"w\" parameter\\n      for i in range(self.parameters[\"w\" + str(l)].shape[0]):\\n        for j in range(self.parameters[\"w\" + str(l).shape[1]]):\\n          self.parameters[\"w\" + str(l)][i][j] += ephsilon  \\n          forward(l, X, Y)\\n          J_plus[j_index++] = compute_cost(Y)\\n          self.parameters[\"w\" + str(l)][i][j] -= ephsilon\\n      # \"b\" parameter\\n      for i in range(self.parameters[\"b\" + str(l)].shape[0]):\\n        for j in range(self.parameters[\"b\" + str(l).shape[1]]):\\n          self.parameters[\"b\" + str(l)][i][j] += ephsilon  \\n          forward(l, X, Y)\\n          J_plus[j_index++] = compute_cost(Y)\\n          self.parameters[\"b\" + str(l)][i][j] -= ephsilon\\n      # compute J minus\\n      # \"w\" parameter\\n      for i in range(self.parameters[\"w\" + str(l)].shape[0]):\\n        for j in range(self.parameters[\"w\" + str(l).shape[1]]):\\n          self.parameters[\"w\" + str(l)][i][j] -= ephsilon  \\n          forward(l, X, Y)\\n          J_minus[j_minus_index++] = compute_cost(Y)\\n          self.parameters[\"w\" + str(l)][i][j] += ephsilon\\n      # \"b\" parameter\\n      for i in range(self.parameters[\"b\" + str(l)].shape[0]):\\n        for j in range(self.parameters[\"b\" + str(l).shape[1]]):\\n          self.parameters[\"b\" + str(l)][i][j] -= ephsilon  \\n          forward(l, X, Y)\\n          J_minus[j_minus_index++] = compute_cost(Y)\\n          self.parameters[\"b\" + str(l)][i][j] += ephsilon\\n    # compute numerical gradient      \\n    grad_numerical = J_plus - J_minus / 2 * ephsilon\\n\\n    # compute backprop gradient\\n    grad_index = 0\\n    for l in range(self.n_layers):\\n      forward(l, X, Y)\\n      backward(l, X, Y)\\n      # collect gradients in an array\\n      # for \\'w\\'\\n      for i in range(self.parameters[\"dw\" + str(l)].shape[0]):\\n        for j in range(self.parameters[\"dw\" + str(l)].shape[1]):\\n          grad_brackprop[grad_index++] = self.parameters[\"dw\" + str(l)][i][j]   \\n      # similarly for \\'b\\'\\n      for i in range(self.parameters[\"db\" + str(l)].shape[0]):\\n        for j in range(self.parameters[\"db\" + str(l)].shape[1]):\\n          grad_brackprop[grad_index] = self.parameters[\"db\" + str(l)][i][j]  \\n    \\n    # compute difference\\n    diff_num = # TODO\\n    diff_den = # TODO\\n    diff = diff_num / diff_den\\n\\n    if diff < threshold:\\n      print(\"Gradients are fine!!\")\\n    else:\\n      print(\"Please check gradients!!\") \\n '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YkAVJgR22rzn",
        "colab": {}
      },
      "source": [
        "llayernn = LLayerNN(n_hidden_units = [10, 2]) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2u5L19ii2wGb",
        "outputId": "4dc32eba-6f00-445e-b47e-638759461f3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        }
      },
      "source": [
        "llayernn.train(x_train_images, y_train_images, learning_rate = 0.0075, \n",
        "               n_epochs = 2500)"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cost at epoch:  0 is:  0.6931471805599453\n",
            "Cost at epoch:  100 is:  0.6780108959202529\n",
            "Cost at epoch:  200 is:  0.6675999388084993\n",
            "Cost at epoch:  300 is:  0.6604220643951918\n",
            "Cost at epoch:  400 is:  0.6554579973905994\n",
            "Cost at epoch:  500 is:  0.6520136220675031\n",
            "Cost at epoch:  600 is:  0.6496159559767932\n",
            "Cost at epoch:  700 is:  0.6479418524023932\n",
            "Cost at epoch:  800 is:  0.6467697350039253\n",
            "Cost at epoch:  900 is:  0.6459470665090711\n",
            "Cost at epoch:  1000 is:  0.6453684178640406\n",
            "Cost at epoch:  1100 is:  0.6449606430092288\n",
            "Cost at epoch:  1200 is:  0.6446728162416298\n",
            "Cost at epoch:  1300 is:  0.6444693704455013\n",
            "Cost at epoch:  1400 is:  0.6443253956966325\n",
            "Cost at epoch:  1500 is:  0.644223403173138\n",
            "Cost at epoch:  1600 is:  0.644151088039195\n",
            "Cost at epoch:  1700 is:  0.6440997768087867\n",
            "Cost at epoch:  1800 is:  0.6440633459354944\n",
            "Cost at epoch:  1900 is:  0.644037466237283\n",
            "Cost at epoch:  2000 is:  0.644019073517953\n",
            "Cost at epoch:  2100 is:  0.6440059967722614\n",
            "Cost at epoch:  2200 is:  0.643996696518268\n",
            "Cost at epoch:  2300 is:  0.6439900803034139\n",
            "Cost at epoch:  2400 is:  0.643985372420969\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0lXrIB7SKpV",
        "colab_type": "text"
      },
      "source": [
        "## Debugging backprop implementation\n",
        "We can get a fair idea of the reasonability of our Backpropogation implementation by comparing it with numerical gradient estimation techniques. Neural network estimates n-dimensional function which maps a input image(in our case) to _cat_ or _no-cat_ label.  \n",
        "In short, we implement the technique described [here](https://youtu.be/QrzApibhohY).  Before I implement this, let us plan:\n",
        "\n",
        "```\n",
        "def convert_dict_to_vec(params_dict):\n",
        "  '''\n",
        "  converts input dictionary to vector  \n",
        "  params_dict: (\"param_string\", param_matrix)\n",
        "  steps:\n",
        "    1. for each element(a matrix) in dictionary:\n",
        "      1.1. compute number of elements in that matrix:\n",
        "         1.1.1. for each element:\n",
        "              1.1.1.1 add that element to params_vect\n",
        "  '''\n",
        "  params_vect = np.array()\n",
        "  return params_vect\n",
        "\n",
        "def convert_vec_to_dict(params_vect):\n",
        "  return params_dict\n",
        "\n",
        "def gradient_check(X, Y, ephsilon, diff_threshold):\n",
        "  parameters = initialize_params()\n",
        "  n_params = count_total_number_of_parameters(parameters)\n",
        "  \n",
        "  # numerical approximation\n",
        "  for param_id in range(n_params):\n",
        "    param_vector = convert_dict_to_vector(parameters)\n",
        "    # cost plus\n",
        "    param_vector[param_id] += epsilon\n",
        "    parameters = convert_vector_to_dict(param_vector)\n",
        "    J_plus = forward(parameters)\n",
        "    # cost_minus\n",
        "    param_vector = convert_dict_to_vector(parameters)\n",
        "    # cost minus\n",
        "    param_vector[param_id] -= epsilon\n",
        "    parameters = convert_vector_to_dict(param_vector)\n",
        "    J_minus = forward(parameters)\n",
        "    grad_numerical[param_id] = (J_plus - J_mins) \\\n",
        "    / 2 * epsilon\n",
        "  \n",
        "  # backprop\n",
        "  forward(parameters)\n",
        "  grad_backprop = backward(parameters)\n",
        "\n",
        "  # compute difference \n",
        "  diff_numerator = np.linalg.norm(grad_backprop \\\n",
        "  - grad_numerical)\n",
        "  diff_denominator = np.linalg.norm(grad_backprop) + \\\n",
        "  np.linalg.norm(grad_numerical) \n",
        "  differance = diff_numerator / diff_denominator\n",
        "\n",
        "  if difference < 2e-7:\n",
        "    print(\"Grad is fine!!\")\n",
        "  else:\n",
        "    print(\"Check Grad!!\")  \n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QO3vLBsBP0W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "llayernn.gradient_check(x_train_images, y_train_images)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}